
# Consumer

- 컨슈머는 풀모델이다.
- 토픽으로부터 데이터를 읽어옴
- 카프카에 데이터를 요청하고 되돌아오는 응답을 받는 구조
    - 카프카가 컨슈머에게 데이터를 주는게 아니라 컨슈머가 직접 요청하는 구조임.
- 컨슈머는 어느 브로커(카프카 서버)에서 데이터를 가져오는지 자동적으로 알게 됨.
- 브로커가 고장이 난 경우에도 컨슈머에서 자체적으로 자동 복구 가능
- 파티션 안에 있는 모든 데이터는 오프셋 순서대로 읽힘

### Consumer Deserialization

- 컨슈머는 직렬화된 데이터를 다시 역직렬화해야됨
- 이 역시 Key, Value별로 deserialize가 따로 있음
- 바이트를 입력으로 받아 키, 벨류를 각각 역직렬화
- Consumer도 동일하게 Common Deserializer제공
    - String (incl. JSON)
    - Int, float
    - Avro
    - Protobuf

즉 Producer - Serializer, Consumer - Deserializer를 갖고 있음

- Consumer가 데이터를 역직렬화 하기 위해서는 키 벨류에 어떤 형식의 Deserializer를 써야할지 미리 알고있어야함
- 따라서 한번 만들고 실행중인 Producer의 Serializer를 절대로 바꿔선 안됨.
    - 따라서 토픽의 자료형을 바꾸고 싶으면 새로운 토픽을 만들자.

# Consumer Group & Offset

- 애플리케이션 내의 데이터를 읽는 모든 Consumer 를 Consumer Group 이라고 한다.

![image.png](Apache%20Kafka%20Series/image%202.png)

- 컨슈머 그룹내에 있는 컨슈머들이 각각 컨슈밍하는 파티션은 서로 겹치지 않음.
- 위처럼 파티션 5개가 있고 컨슈머그룹에 컨슈머가 3개 있으면 위와 같이 파티션이 서로 겹치지 않게 할당됨
- 컨슈머 그룹에 있는 컨슈머가 파티션 개수보다 많으면 어떻게 될까?
    
    ![image.png](Apache%20Kafka%20Series/image%203.png)
    
    - 파티션 개수를 초과한 컨슈머들은 inactive상태가 되고 standby하게 된다.
- Kafka에는 하나의 토픽에 다수의 컨슈머 그룹이 있어도 된다
    
    ![image.png](Apache%20Kafka%20Series/image%204.png)
    
    - 컨슈머 그룹이 여러개 있으니 각각의 파티션에 다수의 리더가 존재
    - 독립적인 컨슈머 그룹을 만들기 위해선 group.id라는 프로퍼티를 이용해 컨슈머 그룹에 이름 지정하고 이를 통해 컨슈머는 자신이 어떤 그룹에 속해있는지 식별함

## Consumer Offset

- 그룹 안에서 컨슈머 Offset을 정의할 수 있음.
    - 카프카는 컨슈머 그룹이 읽고 있던 오프셋을 저장함
    - 오프셋은 __consumer_offsets 라는 kafka 내부 토픽으로 관리됨
- 컨슈머 그룹은 오프셋을 읽다가 주기적으로 오프셋을 커밋하게 됨.
    - 그러면 그 오프셋이 카프카에 저장되고 다음부터는 그 오프셋부터 읽어올 수 있음
- 즉 그룹 안에 있는 컨슈머가 카프카로부터 받은 데이터 처리를 완료하면, 반드시 오프셋을 주기적으로 커밋해야함
    - 커밋한 offset은 그룹에 저장되는게 아니라 별도의 __consumer_offset 내부 토픽에 저장됨
    - 오프셋을 커밋함으로서 Kafka에게 어디까지 성공적으로 읽었는지 알려주는 것
        - 이걸 왜 할까?
            - 컨슈머가 죽었을때 다시 돌아와 커밋한 offset부터 다시 읽기 시작하려고 동기화하는 것임.
            - 장애복구시 사용하는 데이터
- Default 설정으로 Java Consumer는 최소한 한번 이상 자동으로 오프셋을 커밋함
    - 그러나 설정을 통해 이를 3가지 모드로 커밋 방식을 변경 가능
        - 최소한 한 번 - At least once
            - 메시지가 처리된 직후에 메시지가 커밋되는 모드
            - 처리가 잘못되면 메시지는 다시 읽히고 재처리됨
            - 따라서 최소한 한번 설정은 → 같은 메시지를 두 번 읽을 수 있음.
                - 멱등(Idempotent)을 보장해야함 → 동일한 작업을 여러 번 수행해도 결과가 동일하게 유지되어야 함.
        - 최대한 한 번 - At Most Once
            - 컨슈머가 메시지를 받자마자 오프셋을 커밋함
            - 프로세스가 잘못되면 일부 메시지를 읽게됨. (왜냐면 다시 읽지 않아서..)
            - 대신 메시지를 최대 한번만 처리 가능하게됨
        - 정확히 한 번 - Exactly Once
            - 메시지를 딱 한번 처리함
            - Kafka 에서는 Kafka workflow에서 토픽을 읽고 토픽에 다시 쓸때 Transactional API를 이용하면 됨
                - Kafka Streams API쓰면 쉽게 적용가능
            - External System Workflow에서는 idempotnent(멱등) 컨슈머를 사용해야함

# Brokers

- Kafka 클러스터는 다수의 kafka 브로커로 이루어져있음.
    - 브로커는 그냥 카프카 서버를 뜻하는 거임
    - 카프카에서는 브로커라고 부름 - 데이터 보내고 받는걸 중개해서..
- 각 브로커는 고유 ID로 식별 (Integer)
- 각 브로커에는 고유한 토픽 파티션들이 부여됨
    - 즉 모든 데이터가 모든 브로커에 걸쳐서 재분산됨
- 카프카에서 특이한 점
    - 아무 카프카 브로커(부트스트랩 브로커라고 함)에 접속하기만 하면 Kafka 클라이언트, 즉 컨슈머는 전체 카프카 클러스터에 연결됨
    - 즉 카프카 클라이언트는 전체 클러스터에 연결되는 메커니즘을 보유
    - 따라서 클러스터 안의 모든 브로커들을 알 필요 없고 한 브로커에 연결하는 방법만 알면 됨.
- 브로커 개수 제한이 없음
    - 처음에는 3개의 브로커로 시작하는게 좋음. 큰 규모는 100개 넘기도 함.

## Brokers & Topic

- 브로커와 토픽의 관계

![image.png](Apache%20Kafka%20Series/image%205.png)

- 모든 브로커에 걸쳐서 토픽의 파티션들이 각각 할당됨
- 수평적 스케일
    - 파티션과 브로커를 더 추가하면 더 많은 데이터가 전체 클러스터에 걸쳐 분산되기 때문
- 브로커가 모든 데이터를 갖는것은 아님 - 특정 파티션들만 포ㅁ함

## Broker를 찾는 매커니즘

- 모든 카프카 브로커는 bootstrap server이기도 함
- 즉 카프카 내의 특정 브로커만 부트스트랩 서버가 아니라 모든 브로커가 될 수 있음.
    - 따라서 하나의 브로커만 연결하면 됨
- Kafka 클라이언트는 부트스트랩 서버 하나에 연결을 요청하고 Meta 데이터를 요청
    - 연결에 성공하면 클러스터 안의 모든 브로커의 리스트를 리턴함
        - 각 브로커별로 어떤 파티션을 갖는지 등
    - 클라이언트는 이 리스트를 이용해 나머지 필요한 데이터를 상황에 맞는 브로커에게 소비/생산할 수 있음
- 즉 각 브로커는 다른 모든 브로커에 대한 메타데이터를 가지고 있음 (broker, topic, partition)

# Topic replication factor

- 토픽 복제 계수
- 모든 토픽은 반드시 replication factor 계수를 1보다 큰 숫자로 설정해야함 → 보통 2,3 → 3이 젤 많음
- 따라서 브로커가 죽었어도 다른 브로커가 데이터를 관장할 수 있음.

![image.png](Apache%20Kafka%20Series/image%206.png)

- 복제계수를 2개로 설정하면 하나의 토픽의 파티션이 브로커 2개에 복제되어 존재하게됨
- 브로커 102가 죽으면 101,103으로 모든 토픽 데이터 전송 가능
- 즉 복제 계수 2이면 최소 브로커를 1개 잃어도 서비스상 문제가 없다는 뜻
    - 3이면 2개, and so on
    - 여기서 브로커 101, 103이 죽어도 서비스상에는 문제가 없을 것. 그래서 최대 2임.
- 파티션이 복제되는 개념이라 각 파티션에는 리더가 존재하게 됨
    - 파티션별로 리더 파티션은 1개만 존재해야함.
    - Producer는 리더 파티션이 있는 브로커에게만 데이터를 전송할 수 있음
        - 따라서 Broker 101은 파티션 0의 리더이고, 102는 파티션 0의 레플리카이다.
- 만약에 데이터가 충분히 빠르게 복제되면 각각의 레플리카를 ISR- In Sync Replica 라고 부름
    - 반대는 Out-of-Sync Replica

## 리더가 가지고 있는 중요한 특징

- 기본 설정 기준으로 카프카 프로듀서는 오직 리더 파티션의 브로커에게만 데이터 전송 가능
- 카프카 컨슈머도 기본적으로 리더 파티션의 브로커로부터 데이터를 읽어옴
- 즉 replica가 있는 브로커는 진짜 그냥 데이터 복제용이고 장애 났을때 대체하기 위함이다.
    - 리더 파티션이 있는 브로커가 죽으면 복제 파티션이 있는 브로커가 리더가 됨.

### Consumer Replica Fetching

- 카프카 2.4부터 도입된 새로운 기능
- 컨슈머가 자신으로부터 가장 가까운 레플리카 브로커에서 데이터를 읽게 해주는 기능
    - 이를 통해 latency 개선가능
    - 클라우드 환경에서는 네트워크 비용 절약 가능 → 같은 데이터 센터에 레플리카가 있으면 속도가 더 빠르기 때문
- 위와 같은 설정이 있다는 것을 알기만 하면 됨.

# Producer Acknowledgements (acks)

- 프로듀서가 브로커에게 데이터 전송, 브로커 안에는 토픽의 파티션이 있음
- 프로듀서는 데이터 쓰기에 대한 확인을 어떻게 받아올것인지에 대해 선택할 수 있음.
    - 즉 브로커로부터 데이터 쓰기가 잘되었는지 확인을 받을 수 있다는 뜻.
- 3가지 세팅이 있음
    1. acks = 0
        1. 프로듀서가 확인을 기다리거나 요청하지 않음 → 데이터 손실이 발생할 수도 있음.
    2. acks = 1
        1. 프로듀서가 파티션 리더의 확인을 기다림 → 제한적인 데이터 손실 (ISR에서만)
    3. acks = all
        1. 프로듀서가 리더 뿐만아니라 모든 ISR에게 쓰기 확인을 기다림 → 데이터 손실 없음

# Topic Durability

- 위 개념을 통해 토픽 내구성이라는 개념이해 가능
- 토픽 replication factor가 3이면, 토픽 데이터 내구성은 브로커가 2개 잃어도 정상 작동하니 2라고 할 수 있음
- 즉 복제계수가 N이면 N-1개의 브로커가 죽어도 정상 서비스가 가능하고 데이터를 복구할 수 있음
    - 데이터 사본이 클러스터 안의 어딘가 있기 때문

# Zookeeper

- Kafka가 오늘날까지 기능을 유지해온 비결
- 그러나 점점 사라지고 있음 - KRaft로 대체
- Zookeeper는 Kafka 브로커들을 관리하는 소프트웨어
- 브로커가 다운될 때마다 파티션의 새로운 리더를 선출하는데 도움을 줌
- 카프카에게 변경 사항이 있을때마다 카프카 브로커에게 알림을 전송하는 역할
    - 새로운 토픽, 브로커가 죽음, 브로커가 다시 살아남, 토픽 삭제..등..
- 아주 많은 카프카 메타 데이터를 가지고 있음
- Kafka 2.x (~2.8까지)의 버전에는 Zookeeper 없이 동작하지 않음
    - 카프카 출시 시점부터 함께해옴
- Kafka 3.x 부터 Zookeeper없어도 독립적으로 실행 가능
    - 이를 Kafka Raft 매커니즘, 즉 KRaft 라고 부름
    - 궁금하면 KIP-500 치면 나옴
- Kafka 4.x 부터는 Zookeeper 자체를 사용할 수 없게 될 것
- Zookeeper는 홀수 개수의 서버와 함께 동작하도록 설계되어 있음
    - 1, 3, 5, 7 개의 주키퍼 서버(7개를 넘지는 않음)
- Zookeeper도 리더(쓰기)가 1개 있고 나머지는 팔로워(읽기)가 됨
- 카프카 v0.10 전 버전까지는 Zookeeper가 Consumer Offset을 관리했는데 이후 버전부터는 관리하지 않음
    - 즉 주피커는 컨슈머 데이터를 전혀 갖고 있지 않음

![image.png](Apache%20Kafka%20Series/image%207.png)

- 주키퍼 구성은 위처럼 브로커마다 데이터를 전송해주는 주키퍼 서버가 있고 Leader ↔ Follower구조임

## 그렇다면 Zookeeper를 사용해야하는가?

- Kafka Broker를 관리한다면 사용해야함
    - 4.0이 나오기전까지는 프로덕션에서 주키퍼를 사용하는게 맞음
    - 주키퍼 없이 시작할순 있는데 프로덕션 환경에서 검증되진 않았음
- Kafka Client를 관리한다면?
    - 예전에는 프로듀서, 컨슈머, 브로커 할것 없이 모두 주키퍼에 연결했음
    - 그러나 지금은 Client와 Zookeeper를 연결하지 말아야함!!
        - 시간이 지나면서 모든 Kafka 클라이언트와 툴들은 오로지 Broker 엔드포인트와 연결하도록 이전되었기 때문
    - 따라서 컨슈머도 연결하면 안됨 (v0.10부터)
    - [kafka-topic.sh](http://kafka-topic.sh) CLI 커맨드도 주키퍼가 아닌 Kafka 브로커를 참조하도록 바뀜 (v.2.2부터)
        - 토픽 관리하는 CLI 툴임 - creation, deletion 등
        - Zookeeper 와 연결하는 argument는 deprecated 되었음
    - 모든 주키퍼를 사용했던 API와 커맨드도 사용하지 않도록 이전되었음
- 주키퍼가 사라지는 이유는?
    - Kafka보다 안전하지 않기 때문임
        - 만약 사용하게 되면 카프카 클라이언트가 아닌 카프카 브로커로부터의 연결만 수락하도록 해서 주키퍼를 보호
- 결론적으로 모던 카프카를 개발하는데에 있어서 주키퍼는 절때로 클라이언트에 연결하거나 사용하면 안됨! 프로그래밍할때도 마찬가지임

# Kafka KRaft

- 2020년부터 카프카 프로젝트는 Zookeeper 의존성을 제거하기 시작했음(KIP-500)
- 주키퍼를 사용하면 파티션이 100,000개가 넘었을때 스케일링 문제가 발생하였기 때문
    - 주키퍼를 제거함으로써 카프카는 100만개가 넘는 파티션으로 확장가능하게 되고
    - 유지보수와 설정이 쉬워짐, 모니터링하기 쉬워짐
    - 시스템 전체에 하나의 보안모델 사용 가능
    - 카프카를 구동하기 위해 단일 프로세스만 사용하면 됨
    - 따라서 빠른 컨트롤러 셧다운 시간과 복구 시간도 단축됨
- Kafka KRaft는 3.0 버전 부터 구현되었고 3.3.1부터 프로덕션 준비가 되었음 (KIP-833)
- Kafka 4.0부터는 KRaft만 지원될 것
    - 더이상 주키퍼 사용하면 안됨
- KRaft 아키텍쳐에서는 브로커 중 하나가 Quorom Leader가 되어 더 단순화되었음
- 성능도 개선되었음
    - 셧다운 제어 시간과
    - 무제어 셧다운 후 복구 시간이 상당히 개선되었음